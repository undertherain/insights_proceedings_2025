- abstract: Recent advancements in language representation learning primarily emphasize language modeling for deriving meaningful representations, often neglecting style-specific considerations. This study addresses this gap by creating generic, sentence-level style embeddings crucial for style-centric tasks. Our approach is grounded on the premise that low-level text style changes can compose any high-level style. We hypothesize that applying this concept to representation learning enables the development of versatile text style embeddings. By fine-tuning a general-purpose text encoder using contrastive learning and standard cross-entropy loss, we aim to capture these low-level style shifts, anticipating that they offer insights applicable to high-level text styles. The outcomes prompt us to reconsider the underlying assumptions as the results do not always show that the learned style representations capture high-level text styles.
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: ostheimer@cs.uni-kl.de
    first_name: Phil
    institution: RPTU Kaiserslautern-Landau
    last_name: Ostheimer
    name: Phil Ostheimer
    username: philsid
  - emails: kloft@cs.uni-kl.de
    first_name: Marius
    institution: RPTU Kaiserslautern-Landau
    last_name: Kloft
    name: Marius Kloft
    username: kloft
  - emails: fellenz@cs.uni-kl.de
    first_name: Sophie
    institution: RPTU Kaiserslautern-Landau
    last_name: Fellenz
    name: Sophie Fellenz
    username: burkhardt
  decision: Accept to main conference
  file: 1.pdf
  id: '1'
  title: Challenging Assumptions in Learning Generic Text Style Embeddings
- abstract: Few shot in-context learning (ICL) typically assumes access to large annotated training sets. However, in many real world scenarios, such as domain adaptation, there is only a limited budget to annotate a small number of samples, with the goal of maximizing downstream performance. We study various methods for selecting samples to annotate within a predefined budget, focusing on token classification tasks, which are expensive to annotate and are relatively less studied in ICL setups. Across various tasks, models, and datasets, we observe that no method significantly outperforms the others, with most yielding similar results, including random sample selection for annotation. Moreover, we demonstrate that a relatively small annotated sample pool can achieve performance comparable to using the entire training set. We hope that future work adopts our realistic paradigm which takes annotation budget into account.
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: uri.berger2@mail.huji.ac.il
    first_name: Uri
    institution: The Hebrew University of Jerusalem, University of Melbourne
    last_name: Berger
    name: Uri Berger
    username: uriber
  - emails: tabaumel@microsoft.com
    first_name: Tal
    institution: Microsoft
    last_name: Baumel
    name: Tal Baumel
    username: talbaumel
  - emails: gabriel.satanovsky@gmail.com
    first_name: Gabriel
    institution: The Hebrew University of Jerusalem
    last_name: Stanovsky
    name: Gabriel Stanovsky
    username: gabis1986
  decision: Accept to main conference
  file: 2.pdf
  id: '2'
  title: 'In-Context Learning on a Budget: A Case Study in Token Classification'
- abstract: 'Sequence-to-sequence models are widely used to train Abstract Meaning Representation (Banarescu et al.,2013, AMR) parsers. To train such models, AMR graphs have to be linearized into a one-line text format. While Penman encoding is widely used for this purpose, we argue that it has limitations: 1) for deep graphs, some closely related nodes are located far apart in the linearized text 2) Penman''s tree-based encoding necessitates inverse roles to handle node re-entrancy, doubling the number of relation types to predict. To address these issues, we propose a triple-based linearization method and compare its efficiency by training an AMR parser with both approaches. Although triple is well suited to represent a graph, our results show that it does not yet improve performance on deeper or longer graphs. It suggests room for improvement in its design to better compete with Penman''s concise representation and explicit encoding of a nested graph structure.'
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: jeongwoo.kang@univ-grenoble-alpes.fr
    first_name: Jeongwoo
    institution: Université Grenoble Alpes
    last_name: Kang
    name: Jeongwoo Kang
    username: kangje
  - emails: maximin.coavoux@gmail.com
    first_name: Maximin
    institution: CNRS, Univ Grenoble Alpes
    last_name: Coavoux
    name: Maximin Coavoux
    username: mcoavoux
  - emails: didier.schwab@univ-grenoble-alpes.fr
    first_name: Didier
    institution: Univ. Grenoble Alpes
    last_name: Schwab
    name: Didier Schwab
    username: schwab
  - emails: cedric.lopez@emvista.com
    first_name: Cédric
    institution: Emvista
    last_name: Lopez
    name: Cédric Lopez
    username: ''
  decision: Accept to main conference
  file: 4.pdf
  id: '4'
  title: 'Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based'
- abstract: In-context learning (ICL) has transformed the use of large language models (LLMs) for NLP tasks, enabling few-shot learning by conditioning on labeled examples without finetuning. Despite its effectiveness, ICL is prone to errors, especially for challenging examples. With the goal of improving the performance of ICL, we propose *corrective in-context learning* (CICL), an approach that incorporates a model's incorrect predictions alongside ground truth corrections into the prompt, aiming to enhance classification accuracy through self-correction. However, contrary to our hypothesis, extensive experiments on text classification tasks demonstrate that CICL consistently underperforms standard ICL, with performance degrading as the proportion of corrections in the prompt increases. Our findings indicate that CICL introduces confusion by disrupting the model's task understanding, rather than refining its predictions. Additionally, we observe that presenting harder examples in standard ICL does not improve performance, suggesting that example difficulty alone may not be a reliable criterion for effective selection. By presenting these negative results, we provide important insights into the limitations of self-corrective mechanisms in LLMs and offer directions for future research.
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: msanzgue@uni-mainz.de
    first_name: Mario
    institution: Johannes Gutenberg University Mainz
    last_name: S a n z - G u e r r e r o
    name: Mario S a n z - G u e r r e r o
    username: mario-sanz
  - emails: katharina.kann@colorado.edu
    first_name: Katharina
    institution: University of Colorado Boulder
    last_name: Von Der Wense
    name: Katharina Von Der Wense
    username: kann
  decision: Accept to main conference
  file: 5.pdf
  id: '5'
  title: 'Corrective In-Context Learning: Evaluating Self-Correction in Large Language Models'
- abstract: Allocational harms occur when resources or opportunities are unfairly withheld from specific groups. Many proposed bias measures ignore the discrepancy between predictions, which are what the proposed methods consider, and decisions that are made as a result of those predictions. Our work examines the reliability of current bias metrics in assessing allocational harms arising from predictions of large language models (LLMs). We evaluate their predictive validity and utility for model selection across ten LLMs and two allocation tasks. Our results reveal that commonly-used bias metrics based on average performance gap and distribution distance fail to reliably capture group disparities in allocation outcomes. Our work highlights the need to account for how model predictions are used in decisions, in particular in contexts where they are influenced by how limited resources are allocated.
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: yc4dx@virginia.edu
    first_name: Hannah
    institution: University of Virginia
    last_name: Cyberey
    name: Hannah Cyberey
    username: kitcat0618
  - emails: yangfeng@virginia.edu
    first_name: Yangfeng
    institution: University of Virginia
    last_name: Ji
    name: Yangfeng Ji
    username: yangfengji
  - emails: evans@virginia.edu
    first_name: David
    institution: University of Virginia
    last_name: Evans
    name: David Evans
    username: ''
  decision: Accept to main conference
  file: 7.pdf
  id: '7'
  title: Do Prevalent Bias Metrics Capture Allocational Harms from LLMs?
- abstract: 'Multilingual large language models (LLMs) aim towards robust natural language understanding across diverse languages, yet their performance significantly degrades on low-resource languages. This work explores whether existing techniques to identify language-specific neurons can be leveraged to enhance cross-lingual task performance of low-resource languages. We conduct detailed experiments covering existing language-specific neuron identification techniques (such as Language

    Activation Probability Entropy and activation probability-based thresholding) and

    neuron-specific LoRA fine-tuning with models like Llama 3.1 and Mistral Nemo. We find that such neuron-specific interventions are insufficient to yield cross-lingual improvements on downstream tasks (XNLI, XQuAD) in low-resource languages. This study highlights the challenges in achieving cross-lingual generalization and provides critical insights for multilingual LLMs.'
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: soumenkm@iitb.ac.in
    first_name: Soumen Kumar
    institution: IIT Bombay
    last_name: Mondal
    name: Soumen Kumar Mondal
    username: soumenkm
  - emails: sensayam@amazon.com
    first_name: Sayambhu
    institution: Amazon
    last_name: Sen
    name: Sayambhu Sen
    username: sensayam
  - emails: mrabhsin@amazon.com
    first_name: Abhishek
    institution: Amazon
    last_name: Singhania
    name: Abhishek Singhania
    username: mrabhsin
  - emails: pjyothi@cse.iitb.ac.in
    first_name: Preethi
    institution: Indian Institute of Technology Bombay
    last_name: Jyothi
    name: Preethi Jyothi
    username: jyothi
  decision: Accept to main conference
  file: 8.pdf
  id: '8'
  title: Language-Specific Neurons Do Not Facilitate Cross-Lingual Transfer
- abstract: Prior works have shown that in-context learning is brittle to presentation factors such as the order, number, and choice of selected examples. However, ablation-based guidance on selecting the number of examples may ignore the interplay between different presentation factors. In this work we develop a Monte Carlo sampling-based method to study the impact of number of examples while explicitly accounting for effects from order and selected examples. We find that previous guidance on how many in-context examples to select does not always generalize across different sets of selected examples and orderings, and whether one-shot settings outperform zero-shot settings is highly dependent on the selected example. Additionally, inspired by data valuation, we apply our sampling method to in-context example selection to select examples that perform well across different orderings. We find a negative result, that while performance is robust to ordering and number of examples, there is an unexpected performance degradation compared to random sampling.
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: sns2gr@virginia.edu
    first_name: Stephanie
    institution: University of Virginia
    last_name: Schoch
    name: Stephanie Schoch
    username: sschoch
  - emails: yangfeng@virginia.edu
    first_name: Yangfeng
    institution: University of Virginia
    last_name: Ji
    name: Yangfeng Ji
    username: yangfengji
  decision: Accept to main conference
  file: 10.pdf
  id: '10'
  title: Monte Carlo Sampling for Analyzing In-Context Examples
- abstract: An increasingly common practice is to train large language models (LLMs) using synthetic data. Often this synthetic data is produced by the same or similar LLMs as those it is being used to train. This raises the question of whether the synthetic data might in fact exacerbate certain "blindspots'' by reinforcing heuristics that the LLM already encodes. In this paper, we conduct simulated experiments on the natural language inference (NLI) task with Llama-2-7B-hf models. We use MultiNLI as the general task and HANS, a targeted evaluation set designed to measure the presence of specific heuristic strategies for NLI, as our "blindspot'' task. Our goal is to determine whether performance disparities between the general and blind spot tasks emerge. Our results indicate that synthetic data does not reinforce blindspots in the way we expected. Specifically, we see that, while fine-tuning with synthetic data doesn't necessarily reduce the use of the heuristic, it also does not make it worse as we hypothesized.
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: lzjuliuszhang@gmail.com
    first_name: Lingze
    institution: Brown University
    last_name: Zhang
    name: Lingze Zhang
    username: lingzezhang
  - emails: ellie_pavlick@brown.edu
    first_name: Ellie
    institution: Brown University
    last_name: Pavlick
    name: Ellie Pavlick
    username: epavlick
  decision: Accept to main conference
  file: 11.pdf
  id: '11'
  title: Does Training on Synthetic Data Make Models Less Robust?
- abstract: Prototypical Network-based Language Models (PNLMs) have been introduced as a novel approach for enhancing interpretability in deep learning models for NLP. In this work, we show that, despite the transparency afforded by their case-based reasoning architecture, current PNLMs are, in fact, not faithful, i.e. their explanations do not accurately reflect the underlying model's reasoning process. By adopting an axiomatic approach grounded in the seminal works' definition of faithfulness, we identify two specific points in the architecture of PNLMs where unfaithfulness may occur. To address this, we introduce Faithful Alignment (FA), a two-part framework that ensures the faithfulness of PNLMs' explanations. We then demonstrate that FA achieves this goal without compromising model performance across a variety of downstream tasks and ablation studies.
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: akouloge@andrew.cmu.edu
    first_name: Andrew
    institution: Carnegie Mellon University
    last_name: Koulogeorge
    name: Andrew Koulogeorge
    username: andrew_koulogeorge
  - emails: sean.xie.gr@dartmouth.edu
    first_name: Sean
    institution: Dartmouth College
    last_name: Xie
    name: Sean Xie
    username: sean.xie
  - emails: saeed.hassanpour@dartmouth.edu
    first_name: Saeed
    institution: Dartmouth College
    last_name: Hassanpour
    name: Saeed Hassanpour
    username: saeed.hassanpour
  - emails: soroush@dartmouth.edu
    first_name: Soroush
    institution: Dartmouth
    last_name: Vosoughi
    name: Soroush Vosoughi
    username: soroushv
  decision: Accept to main conference
  file: 12.pdf
  id: '12'
  title: Bridging the Faithfulness Gap in Prototypical Models
- abstract: 'Intermediate Layer Distillation (ILD) is a variant of Knowledge Distillation (KD), a method for compressing neural networks.

    ILD requires mapping to align the intermediate layer sizes of the teacher and student models to compute the loss function in training, while this mapping is not used during inference.

    This inconsistency may reduce the effectiveness of learning in intermediate layers.

    In this study, we propose LoRAILD, which uses LoRA adapters to eliminate the inconsistency.

    However, our experimental results show that LoRAILD does not outperform existing methods.

    Furthermore, contrary to previous studies, we observe that conventional ILD does not outperform vanilla KD.

    Our analysis of the distilled models'' intermediate layers suggests that ILD does not improve language models'' performance.'
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: suzuki.t.dp@m.titech.ac.jp
    first_name: Takeshi
    institution: Institute of Science Tokyo
    last_name: Suzuki
    name: Takeshi Suzuki
    username: tkc002
  - emails: yamada@comp.isct.ac.jp
    first_name: Hiroaki
    institution: Institute of Science Tokyo
    last_name: Yamada
    name: Hiroaki Yamada
    username: h_yamada
  - emails: take@c.titech.ac.jp
    first_name: Takenobu
    institution: Institute of Science Tokyo
    last_name: Tokunaga
    name: Takenobu Tokunaga
    username: take
  decision: Accept to main conference
  file: 14.pdf
  id: '14'
  title: Aligning Sizes of Intermediate Layers by LoRA Adapter for Knowledge Distillation
- abstract: Large Language Models (LLMs) are increasingly adopted for applications in healthcare, reaching the performance of domain experts on tasks such as question answering and document summarisation. Despite their success on these tasks, it is unclear how well LLMs perform on tasks that are traditionally pursued in the biomedical domain, such as structured information extration. To bridge this gap, in this paper, we systematically benchmark LLM performance in Medical Classification and Named Entity Recognition (NER) tasks. We aim to disentangle the contribution of different factors to the performance, particularly the impact of LLMs' task knowledge and reasoning capabilities, their (parametric) domain knowledge, and addition of external knowledge. To this end, we evaluate various open LLMs---including BioMistral and Llama-2 models---on a diverse set of biomedical datasets, using standard prompting, Chain-of-Thought (CoT) and Self-Consistency based reasoning as well as Retrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora. Counter-intuitively, our results reveal that standard prompting consistently outperforms more complex techniques across both tasks, laying bare the limitations in the current application of CoT, self-consistency and RAG in the biomedical domain. Our findings suggest that advanced prompting methods developed for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are not easily portable to biomedical tasks where precise structured outputs are required. This highlights the need for more effective integration of external knowledge and reasoning mechanisms in LLMs to enhance their performance in real-world biomedical applications.
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: aishiknagar@gmail.com
    first_name: Aishik
    institution: ASUS Global Pte. Ltd.
    last_name: Nagar
    name: Aishik Nagar
    username: aishiknagar
  - emails: viktor_schlegel@asus.com
    first_name: Viktor
    institution: ASUS AICS
    last_name: Schlegel
    name: Viktor Schlegel
    username: schlevik
  - emails: tungnguyen0424@gmail.com
    first_name: T h a n h - T u n g
    institution: ASUS
    last_name: Nguyen
    name: T h a n h - T u n g Nguyen
    username: tungngthanh
  - emails: hao.li-2@manchester.ac.uk
    first_name: Hao
    institution: University of Manchester
    last_name: Li
    name: Hao Li
    username: haoli-2
  - emails: yuping.wu@manchester.ac.uk
    first_name: Yuping
    institution: University of Manchester
    last_name: Wu
    name: Yuping Wu
    username: yuping.wu
  - emails: kuluhan@comp.nus.edu.sg
    first_name: Kuluhan
    institution: National University of Singapore
    last_name: Binici
    name: Kuluhan Binici
    username: kuluhan
  - emails: winkler@nus.edu.sg
    first_name: Stefan
    institution: ASUS Intelligent Cloud Services (AICS)
    last_name: Winkler
    name: Stefan Winkler
    username: stefanwinkler
  decision: Accept to main conference
  file: 16.pdf
  id: '16'
  title: LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction
- abstract: We propose using prompts made up of multiple problems to evaluate LLM capabilities, an approach we call multi-problem evaluation. We examine 7 LLMs on 4 related task types constructed from 6 existing classification benchmarks. We find that while LLMs can generally perform multiple homogeneous classifications at once (Batch Classification) as well as when they do so separately, they perform significantly worse on two selection tasks that are conceptually equivalent to Batch Classification and involve selecting indices of text falling into each class label, either independently or altogether. We show that such a significant performance drop is due to LLMs' inability to adequately combine index selection with text classification. Such a drop is surprisingly observed across all LLMs attested, under zero-shot, few-shot, and CoT settings, and even with a novel synthetic dataset, potentially reflecting an inherent capability limitation with modern LLMs.
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: zhengxiang.wang@stonybrook.edu
    first_name: Zhengxiang
    institution: Stony Brook University
    last_name: Wang
    name: Zhengxiang Wang
    username: zhengxiang.wang
  - emails: jordan.kodner@stonybrook.edu
    first_name: Jordan
    institution: Stony Brook University
    last_name: Kodner
    name: Jordan Kodner
    username: ''
  - emails: owen.rambow@stonybrook.edu
    first_name: Owen
    institution: Stony Brook University
    last_name: Rambow
    name: Owen Rambow
    username: ''
  decision: Accept to main conference
  file: 18.pdf
  id: '18'
  title: Exploring Limitations of LLM Capabilities with Multi-Problem Evaluation
- abstract: Sustainability metrics have increasingly become a crucial non-financial criterion in investment decision-making. Organizations worldwide are recognizing the importance of sustainability and are proactively highlighting their efforts through specialized sustainability reports. Unlike traditional annual reports, these sustainability disclosures are typically text-heavy and are often expressed as infographics, complex tables, and charts. The non-machine-readable nature of these reports presents a significant challenge for efficient information extraction. The rapid advancement of Vision Language Models (VLMs) has raised the question whether these VLMs can address such challenges in domain specific task. In this study, we demonstrate the application of VLMs for extracting sustainability information from dedicated sustainability reports. Our experiments highlight the limitations in the performance of several open-source VLMs in extracting information about sustainability disclosures from different type of pages.
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: gupta.tanay@tcs.com
    first_name: Tanay
    institution: TCS Research
    last_name: Gupta
    name: Tanay Gupta
    username: ''
  - emails: t.goel@tcs.com
    first_name: Tushar
    institution: TCS Research
    last_name: Goel
    name: Tushar Goel
    username: ''
  - emails: ishan.verma@tcs.com
    first_name: Ishan
    institution: TCS Research
    last_name: Verma
    name: Ishan Verma
    username: ishanverma
  decision: Accept to main conference
  file: 19.pdf
  id: '19'
  title: 'Exploring Multimodal Language Models for Sustainability Disclosure Extraction: A Comparative Study'
- abstract: Large Language Models (LLMs) enhanced with tool use and APIs improve task performance but often misuse them, leading to inefficiency and unnecessary cost. We propose Self Knowledge-Tracing for Tool Use (SKT-Tool), a method enabling LLMs to assess their capabilities and make informed API usage decisions using knowledge tracing (KT). Our teacher-student framework helps LLMs optimize API calls in real-time without fine-tuning. Experiments across multiple datasets show that SKT-Tool significantly reduces API calls while maintaining accuracy, offering a scalable and cost-effective solution for tool-augmented LLMs. We conclude by analyzing shortcomings in this method and identifying directions for future work.
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: joshvigel@gmail.com
    first_name: Joshua
    institution: Algoverse
    last_name: Vigel
    name: Joshua Vigel
    username: ''
  - emails: nasevate@gmail.com
    first_name: Renpei
    institution: Algoverse
    last_name: Cai
    name: Renpei Cai
    username: cairenpei
  - emails: tarobrightlynx@gmail.com
    first_name: Eleanor
    institution: Algoverse
    last_name: Chen
    name: Eleanor Chen
    username: ''
  - emails: anish.neema2@gmail.com
    first_name: Anish
    institution: Algoverse
    last_name: Neema
    name: Anish Neema
    username: ''
  - emails: austenliao@berkeley.edu
    first_name: Austen
    institution: Algoverse
    last_name: Liao
    name: Austen Liao
    username: ''
  - emails: zhu502846@berkeley.edu
    first_name: Kevin
    institution: Algoverse
    last_name: Zhu
    name: Kevin Zhu
    username: kevinzhu
  - emails: seobrien@ucsd.edu
    first_name: Sean
    institution: Algoverse
    last_name: O'brien
    name: Sean O'brien
    username: ''
  decision: Accept to main conference
  file: 23.pdf
  id: '23'
  title: 'Self Knowledge-Tracing for Tool Use (SKT-Tool): Helping LLM Agents Understand Their Capabilities in Tool Use'
- abstract: Prompting methods for language models, such as Chain-of-thought (CoT), present intuitive step-by-step processes for problem solving. These methodologies aim to equip models with a better understanding of the correct procedures for addressing a given task. Despite these advancements, CoT lacks the ability of reflection and error correction, potentially causing a model to perpetuate mistakes and errors. Therefore, inspired by the human ability for said tasks, we propose Error Reflection Prompting (ERP) to further enhance reasoning in language models. Building upon CoT, ERP is a method comprised of an incorrect answer, error recognition, and a correct answer. This process enables the model to recognize types of errors and the steps that lead to incorrect answers, allowing the model to better discern which steps to avoid and which to take. The model is able to generate the error outlines itself with automated ERP generation, allowing for error recognition and correction to be integrated into the reasoning chain and produce scalability and reliability in the process. The results demonstrate that ERP serves as a versatile supplement to conventional CoT, ultimately contributing to more robust and capable reasoning abilities along with increased interpretability in how models ultimately reach their errors.
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: jl255788@gmail.com
    first_name: Jason
    institution: Algoverse
    last_name: Li
    name: Jason Li
    username: ''
  - emails: s.lauren.yraola@gmail.com
    first_name: Lauren
    institution: Algoverse
    last_name: Yraola
    name: Lauren Yraola
    username: ''
  - emails: zhu502846@berkeley.edu
    first_name: Kevin
    institution: Algoverse
    last_name: Zhu
    name: Kevin Zhu
    username: kevinzhu
  - emails: seobrien@ucsd.edu
    first_name: Sean
    institution: Algoverse
    last_name: O'brien
    name: Sean O'brien
    username: ''
  decision: Accept to main conference
  file: 24.pdf
  id: '24'
  title: 'Error Reflection Prompting: Can Large Language Models Successfully Understand Errors?'
- abstract: Evaluating an LLM's robustness against numerical perturbation is a good way to know if the LLM actually performs reasoning or just replicates patterns learned. We propose a novel method to augment math word problems (MWPs), producing numerical variations at a large scale utilizing templates. We also propose an automated error classification framework for scalable error analysis, distinguishing calculation errors from reasoning errors. Our experiments using the methods show LLMs are weak against numerical variations, suggesting they are not fully capable of generating valid reasoning steps, often failing in arithmetic operations.
  attributes:
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: yang.y.aw@m.titech.ac.jp
    first_name: Yuli
    institution: Institute of Science Tokyo
    last_name: Yang
    name: Yuli Yang
    username: ''
  - emails: yamada@comp.isct.ac.jp
    first_name: Hiroaki
    institution: Institute of Science Tokyo
    last_name: Yamada
    name: Hiroaki Yamada
    username: h_yamada
  - emails: take@c.titech.ac.jp
    first_name: Takenobu
    institution: Institute of Science Tokyo
    last_name: Tokunaga
    name: Takenobu Tokunaga
    username: take
  decision: Accept to main conference
  file: 25.pdf
  id: '25'
  title: Evaluating Robustness of LLMs to Numerical Variations in Mathematical Reasoning
